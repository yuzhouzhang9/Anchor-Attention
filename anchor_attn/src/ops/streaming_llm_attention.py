
import math
from typing import Optional

import torch
import triton
import triton.language as tl


def torch_streaming_llm_attention(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    global_window: int,
    local_window: int,
    softmax_scale: Optional[float] = None,
    gqa_interleave: bool = False,
):
    b, n, h, d = q.shape
    gqa_groups = q.shape[2] // k.shape[2]
    if gqa_groups > 1 and not gqa_interleave:
        k = k.repeat_interleave(gqa_groups, 2)
        v = v.repeat_interleave(gqa_groups, 2)
    elif gqa_groups > 1 and gqa_interleave:
        k = k.repeat(1, 1, gqa_groups, 1)
        v = v.repeat(1, 1, gqa_groups, 1)
    assert k.shape == q.shape
    assert v.shape == k.shape
    if softmax_scale is None:
        softmax_scale = 1 / math.sqrt(d)
    attn_weight = torch.einsum("bihd,bjhd->bihj", q, k) * softmax_scale
    range_n = torch.arange(n)
    mask = (range_n[:, None] >= range_n[None, :]) & (
        (range_n[:, None] < range_n[None, :] + local_window)
        | (range_n[None, :] < global_window)
    )
    mask = mask.view(1, n, 1, n).to(q.device)
    attn_weight.masked_fill_(~mask, float("-inf"))
    attn_weight = torch.softmax(attn_weight, dim=-1)
    o = torch.einsum("bihj,bjhd->bihd", attn_weight, v)
    return o


@triton.jit
def streaming_attention_kernel(
    q_ptr,
    k_ptr,
    v_ptr,
    o_ptr,
    # shape
    BATCH_SIZE,
    NUM_HEADS,
    NUM_KV_HEADS,
    GQA_GROUPS,
    Q_LEN,
    K_LEN,
    HEAD_DIM: tl.constexpr,
    GLOBAL_WINDOW,
    LOCAL_WINDOW,
    # softmax_scale
    softmax_scale,
    # gqa
    gqa_interleave: tl.constexpr,
    # stride
    stride_qb,
    stride_qn,
    stride_qh,
    stride_qd,
    stride_kb,
    stride_kn,
    stride_kh,
    stride_kd,
    stride_vb,
    stride_vn,
    stride_vh,
    stride_vd,
    stride_ob,
    stride_on,
    stride_oh,
    stride_od,
    # META parameters
    BLOCK_SIZE_Q: tl.constexpr,  # q block size
    BLOCK_SIZE_K: tl.constexpr,  # k block size
):
    # get batch id and head id
    pid_q = tl.program_id(0)
    pid_bh = tl.program_id(1)
    pid_b = pid_bh // NUM_HEADS
    pid_h = pid_bh % NUM_HEADS
    if gqa_interleave:
        pid_kh = pid_h % NUM_KV_HEADS
    else:
        pid_kh = pid_h // GQA_GROUPS
    # init qkv pointer
    q_ptrs = tl.make_block_ptr(
        base=q_ptr + pid_b * stride_qb + pid_h * stride_qh,
        shape=(Q_LEN, HEAD_DIM),
        strides=(stride_qn, stride_qd),
        offsets=(pid_q * BLOCK_SIZE_Q, 0),
        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),
        order=(1, 0),
    )
    # load q
    q = tl.load(q_ptrs, boundary_check=(0, 1), padding_option="zero")
    # init statistics
    off_m = tl.arange(0, BLOCK_SIZE_Q) + pid_q * BLOCK_SIZE_Q
    off_n = tl.arange(0, BLOCK_SIZE_K)
    m_i = tl.full((BLOCK_SIZE_Q,), float("-inf"), dtype=tl.float32)
    lse_i = tl.full((BLOCK_SIZE_Q,), float("-inf"), dtype=tl.float32)
    acc_o = tl.full((BLOCK_SIZE_Q, HEAD_DIM), 0, dtype=tl.float32)
    # global window
    lo = 0
    hi = GLOBAL_WINDOW
    k_ptrs = tl.make_block_ptr(
        base=k_ptr + pid_b * stride_kb + pid_kh * stride_kh,
        shape=(HEAD_DIM, K_LEN),
        strides=(stride_kd, stride_kn),
        offsets=(0, lo),
        block_shape=(HEAD_DIM, BLOCK_SIZE_K),
        order=(0, 1),
    )
    v_ptrs = tl.make_block_ptr(
        base=v_ptr + pid_b * stride_vb + pid_kh * stride_vh,
        shape=(K_LEN, HEAD_DIM),
        strides=(stride_vn, stride_vd),
        offsets=(lo, 0),
        block_shape=(BLOCK_SIZE_K, HEAD_DIM),
        order=(1, 0),
    )
    for i in tl.range(lo, hi, BLOCK_SIZE_K):
        i = tl.multiple_of(i, BLOCK_SIZE_K)
        # load k
        k = tl.load(k_ptrs, boundary_check=(1,), padding_option="zero")
        # compute qk
        qk = tl.zeros((BLOCK_SIZE_Q, BLOCK_SIZE_K), dtype=tl.float32)
        qk_mask = (off_m[:, None] >= (i + off_n)[None, :]) & (  # causal mask
            (off_m[:, None] < (i + LOCAL_WINDOW + off_n)[None, :])  # local mask
            | ((i + off_n)[None, :] < GLOBAL_WINDOW)  # global mask
        )
        qk += tl.where(qk_mask, 0, float("-inf"))
        qk += tl.dot(q, k) * softmax_scale
        # compute m_ij and l_ij
        m_ij = tl.maximum(m_i, tl.max(qk, axis=1))
        p = tl.math.exp2(qk - m_ij[:, None])
        p = tl.where(qk_mask, p, 0)  # mask p where qk and m_if are both -inf
        l_ij = tl.sum(p, axis=1)
        # scale acc_o
        acc_o_scale = tl.math.exp2(m_i - m_ij)
        acc_o_scale = tl.where(
            tl.max(qk_mask, axis=1), acc_o_scale, 1
        )  # mask p where m_i and m_if are both -inf
        acc_o = acc_o * acc_o_scale[:, None]
        # load v and update acc_o
        v = tl.load(v_ptrs, boundary_check=(0,), padding_option="zero")
        p = p.to(v.dtype)
        acc_o += tl.dot(p, v)
        # update statistics
        m_i = m_ij
        lse_i = m_ij + tl.math.log2(tl.math.exp2(lse_i - m_ij) + l_ij)
        # update ptrs
        k_ptrs = tl.advance(k_ptrs, (0, BLOCK_SIZE_K))
        v_ptrs = tl.advance(v_ptrs, (BLOCK_SIZE_K, 0))
    # local window
    lo = (pid_q * BLOCK_SIZE_Q - LOCAL_WINDOW + 1) // BLOCK_SIZE_K * BLOCK_SIZE_K
    lo = max(tl.cdiv(GLOBAL_WINDOW, BLOCK_SIZE_K) * BLOCK_SIZE_K, lo)
    hi = (pid_q + 1) * BLOCK_SIZE_Q
    k_ptrs = tl.make_block_ptr(
        base=k_ptr + pid_b * stride_kb + pid_kh * stride_kh,
        shape=(HEAD_DIM, K_LEN),
        strides=(stride_kd, stride_kn),
        offsets=(0, lo),
        block_shape=(HEAD_DIM, BLOCK_SIZE_K),
        order=(0, 1),
    )
    v_ptrs = tl.make_block_ptr(
        base=v_ptr + pid_b * stride_vb + pid_kh * stride_vh,
        shape=(K_LEN, HEAD_DIM),
        strides=(stride_vn, stride_vd),
        offsets=(lo, 0),
        block_shape=(BLOCK_SIZE_K, HEAD_DIM),
        order=(1, 0),
    )
    for i in tl.range(lo, hi, BLOCK_SIZE_K):
        i = tl.multiple_of(i, BLOCK_SIZE_K)
        # load k
        k = tl.load(k_ptrs, boundary_check=(1,), padding_option="zero")
        # compute qk
        qk = tl.zeros((BLOCK_SIZE_Q, BLOCK_SIZE_K), dtype=tl.float32)
        qk_mask = (off_m[:, None] >= (i + off_n)[None, :]) & (  # causal mask
            (off_m[:, None] < (i + LOCAL_WINDOW + off_n)[None, :])  # local mask
            | ((i + off_n)[None, :] < GLOBAL_WINDOW)  # global mask
        )
        qk += tl.where(qk_mask, 0, float("-inf"))
        qk += tl.dot(q, k) * softmax_scale
        # compute m_ij and l_ij
        m_ij = tl.maximum(m_i, tl.max(qk, axis=1))
        p = tl.math.exp2(qk - m_ij[:, None])
        p = tl.where(qk_mask, p, 0)  # mask p where qk and m_if are both -inf
        l_ij = tl.sum(p, axis=1)
        # scale acc_o
        acc_o_scale = tl.math.exp2(m_i - m_ij)
        acc_o_scale = tl.where(
            tl.max(qk_mask, axis=1), acc_o_scale, 1
        )  # mask p where m_i and m_if are both -inf
        acc_o = acc_o * acc_o_scale[:, None]
        # load v and update acc_o
        v = tl.load(v_ptrs, boundary_check=(0,), padding_option="zero")
        p = p.to(v.dtype)
        acc_o += tl.dot(p, v)
        # update statistics
        m_i = m_ij
        lse_i = m_ij + tl.math.log2(tl.math.exp2(lse_i - m_ij) + l_ij)
        # update ptrs
        k_ptrs = tl.advance(k_ptrs, (0, BLOCK_SIZE_K))
        v_ptrs = tl.advance(v_ptrs, (BLOCK_SIZE_K, 0))
    # final scale
    acc_o = acc_o * tl.math.exp2(m_i - lse_i)[:, None]
    # save output
    o_ptrs = tl.make_block_ptr(
        base=o_ptr + pid_b * stride_ob + pid_h * stride_oh,
        shape=(Q_LEN, HEAD_DIM),
        strides=(stride_on, stride_od),
        offsets=(pid_q * BLOCK_SIZE_Q, 0),
        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),
        order=(1, 0),
    )
    tl.store(o_ptrs, acc_o.to(tl.bfloat16), boundary_check=(0,))




def streaming_llm_attention(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    global_window: int,
    local_window: int,
    softmax_scale: Optional[float] = None,
    gqa_interleave: bool = False,
):
    batch_size, q_len, num_q_heads, head_dim = q.shape
    batch_size, k_len, num_kv_heads, head_dim = k.shape
    assert v.shape == k.shape
    assert q.dtype == torch.bfloat16, "only support dtype bfloat16"
    assert head_dim in {16, 32, 64, 128}, "only support head_dim in {16, 32, 64, 128}"
    # gqa
    assert num_q_heads % num_kv_heads == 0
    gqa_groups = num_q_heads // num_kv_heads
    # softmax_scale needs to be multiplied by math.log2(math.e)
    if softmax_scale is None:
        softmax_scale = 1 / math.sqrt(head_dim) * math.log2(math.e)
    else:
        softmax_scale = softmax_scale * math.log2(math.e)
    # output tensor
    o = torch.empty_like(q)
    grid = lambda META: (
        triton.cdiv(q_len, META["BLOCK_SIZE_Q"]),
        batch_size * num_q_heads,
    )
    # set num_warps=4 if headdim=64 and num_warps=8 if headdim=128
    num_warps = 4 if head_dim <= 64 else 8
    BLOCK_SIZE_Q = 128
    BLOCK_SIZE_K = 128
    streaming_attention_kernel[grid](
        q,
        k,
        v,
        o,
        batch_size,
        num_q_heads,
        num_kv_heads,
        gqa_groups,
        q_len,
        k_len,
        head_dim,
        global_window,
        local_window,
        softmax_scale,
        gqa_interleave,
        q.stride(0),
        q.stride(1),
        q.stride(2),
        q.stride(3),
        k.stride(0),
        k.stride(1),
        k.stride(2),
        k.stride(3),
        v.stride(0),
        v.stride(1),
        v.stride(2),
        v.stride(3),
        o.stride(0),
        o.stride(1),
        o.stride(2),
        o.stride(3),
        BLOCK_SIZE_Q=BLOCK_SIZE_Q,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        num_warps=num_warps,
        num_stages=3,
    )
    return o


if __name__ == "__main__":
    from termcolor import colored

    from anchor_attn.src.ops.flex_prefill_attention import triton_flash_attention

    torch.manual_seed(42)

    print("** streaming llm attention: **")
    B, N, H, D = 1, 8000, 32, 128
    G, L = 128, 512
    Q = torch.randn(B, N, H, D, device="cuda", dtype=torch.bfloat16)
    K = torch.randn(B, N, H, D, device="cuda", dtype=torch.bfloat16)
    V = torch.randn(B, N, H, D, device="cuda", dtype=torch.bfloat16)
    output_torch = torch_streaming_llm_attention(Q, K, V, G, L)
    output_triton = streaming_llm_attention(Q, K, V, G, L)
    if torch.allclose(output_torch, output_triton, rtol=1e-2, atol=1e-2):
        print(colored("triton output == torch output:", "green"))
    else:
        print(colored("triton output != torch output:", "red"))
    print(
        f"The maximum difference between torch and triton is "
        f"{torch.max(torch.abs(output_torch - output_triton))}"
    )
    tol = 0.01
    err_num = torch.where(torch.abs(output_torch - output_triton) > tol)[0].numel()
    print(
        f"number of elements whose error > {tol}: {err_num} ({err_num / output_torch.numel()*100:.2f}%)",
    )
    print()
    # gqa attention test
    print("** gqa streaming llm attention: **")
    Q = torch.randn(B, N, H, D, device="cuda", dtype=torch.bfloat16)
    K = torch.randn(B, N, H // 4, D, device="cuda", dtype=torch.bfloat16)
    V = torch.randn(B, N, H // 4, D, device="cuda", dtype=torch.bfloat16)
    output_torch = torch_streaming_llm_attention(Q, K, V, G, L)
    output_triton = streaming_llm_attention(Q, K, V, G, L)
    if torch.allclose(output_torch, output_triton, rtol=1e-2, atol=1e-2):
        print(colored("triton output == torch output:", "green"))
    else:
        print(colored("triton output != torch output:", "red"))
    print(
        f"The maximum difference between torch and triton is "
        f"{torch.max(torch.abs(output_torch - output_triton))}"
    )
    tol = 0.01
    err_num = torch.where(torch.abs(output_torch - output_triton) > tol)[0].numel()
    print(
        f"number of elements whose error > {tol}: {err_num} ({err_num / output_torch.numel()*100:.2f}%)",
    )
    print()

    # benchmark
    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=["N"],
            x_vals=[1024 * 2**i for i in range(1, 8)],
            line_arg="provider",
            line_vals=["triton-full", "triton-streaming"],
            line_names=[
                "triton-full",
                "triton-streaming",
            ],
            styles=[("green", "-"), ("blue", "-")],
            ylabel="ms",
            plot_name="** streaming llm attention performance (ms) **",
            args={"B": 1, "H": 32, "D": 128, "G": 128, "L": 512},
        )
    )
    def benchmark(B, N, H, D, G, L, provider):
        q = torch.randn((B, N, H, D), device="cuda", dtype=torch.bfloat16)
        k = torch.randn((B, N, H, D), device="cuda", dtype=torch.bfloat16)
        v = torch.randn((B, N, H, D), device="cuda", dtype=torch.bfloat16)
        quantiles = [0.5, 0.2, 0.8]
        if provider == "triton-full":
            ms, min_ms, max_ms = triton.testing.do_bench(
                lambda: triton_flash_attention(q, k, v), quantiles=quantiles
            )
        if provider == "triton-streaming":
            ms, min_ms, max_ms = triton.testing.do_bench(
                lambda: streaming_llm_attention(q, k, v, G, L),
                quantiles=quantiles,
            )
        return ms, min_ms, max_ms

    benchmark.run(show_plots=True, print_data=True)
    print()
