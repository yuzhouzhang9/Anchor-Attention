import math
import random
from typing import List, Optional, Union

import torch
import triton
import triton.language as tl
from einops import rearrange
from flash_attn import flash_attn_func
from my_utils import Logger
from anchor_attn.src.ops.flex_prefill_attention import triton_block_wise_attention


def get_vertical_slash_block_num(
    vertical_index: torch.Tensor, slash_index: torch.Tensor, num_blocks: int
):
    batch_size, num_heads, v_blocks = vertical_index.shape
    batch_size, num_heads, s_blocks = slash_index.shape
    range_blocks = torch.arange(
        num_blocks, device=vertical_index.device, dtype=torch.int32
    )
    slash_num_blocks = (
        slash_index[..., None, :] <= range_blocks[None, None, :, None]
    ).sum(-1)
    vertical_num_blocks = (
        vertical_index[..., None, :] <= range_blocks[None, None, :, None]
    ).sum(-1)
    return vertical_num_blocks.to(torch.int32), slash_num_blocks.to(torch.int32)


@triton.jit
def vertical_slash_attention_kernel(
    q_ptr,  # shape: [batch_size, seq_len, num_q_heads, head_dim]
    k_ptr,  # shape: [batch_size, seq_len, num_kv_heads, head_dim]
    v_ptr,  # shape: [batch_size, seq_len, num_kv_heads, head_dim]
    o_ptr,  # shape: [batch_size, seq_len, num_q_heads, head_dim]
    v_idx_ptr,  # shape: [batch_size, num_heads, num_v_blocks]
    v_num_ptr,  # shape: [batch_size, num_heads, num_rows]
    s_idx_ptr,  # shape: [batch_size, num_heads, num_rows]
    s_num_ptr,  # shape: [batch_size, num_heads]
    # shape
    BATCH_SIZE,
    NUM_Q_HEADS,
    NUM_KV_HEADS,
    GQA_GROUPS,
    Q_LEN,
    K_LEN,
    HEAD_DIM: tl.constexpr,
    # softmax_scale
    softmax_scale,
    # gqa
    gqa_interleave: tl.constexpr,
    # stride
    stride_qb,
    stride_qn,
    stride_qh,
    stride_qd,
    stride_kb,
    stride_kn,
    stride_kh,
    stride_kd,
    stride_vb,
    stride_vn,
    stride_vh,
    stride_vd,
    stride_ob,
    stride_on,
    stride_oh,
    stride_od,
    # vertical index stride
    stride_vib,
    stride_vih,
    stride_vit,
    stride_vnb,
    stride_vnh,
    stride_vnt,
    # slash index stride
    stride_sib,
    stride_sih,
    stride_sit,
    stride_snb,
    stride_snh,
    stride_snt,
    # META parameters
    BLOCK_SIZE_Q: tl.constexpr,  # q block size
    BLOCK_SIZE_K: tl.constexpr,  # k block size

):
    tl.static_assert(BLOCK_SIZE_Q == BLOCK_SIZE_K)
    pid_b = tl.program_id(1)
    pid_h = tl.program_id(2)
    if gqa_interleave:
        pid_kh = pid_h % NUM_KV_HEADS
    else:
        pid_kh = pid_h // GQA_GROUPS
    pid_q = tl.program_id(0)
    # get vertical block num and slash block num
    v_num_ptr = v_num_ptr + pid_b * stride_vnb + pid_h * stride_vnh
    v_num = tl.load(v_num_ptr + pid_q * stride_vnt)
    s_num_ptr = s_num_ptr + pid_b * stride_snb + pid_h * stride_snh
    s_num = tl.load(s_num_ptr + pid_q * stride_snt)
    # init v_count and s_count
    v_idx_ptr = v_idx_ptr + pid_b * stride_vib + pid_h * stride_vih
    s_idx_ptr = (
        s_idx_ptr + pid_b * stride_sib + pid_h * stride_sih + (s_num - 1) * stride_sit
    )
    v_count = 0
    s_count = 0
    # init qkv ptrs
    q_ptrs = tl.make_block_ptr(
        base=q_ptr + pid_b * stride_qb + pid_h * stride_qh,
        shape=(Q_LEN, HEAD_DIM),
        strides=(stride_qn, stride_qd),
        offsets=(pid_q * BLOCK_SIZE_Q, 0),
        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),
        order=(1, 0),
    )
    k_ptrs = tl.make_block_ptr(
        base=k_ptr + pid_b * stride_kb + pid_kh * stride_kh,
        shape=(HEAD_DIM, K_LEN),
        strides=(stride_kd, stride_kn),
        offsets=(0, 0),
        block_shape=(HEAD_DIM, BLOCK_SIZE_K),
        order=(0, 1),
    )
    v_ptrs = tl.make_block_ptr(
        base=v_ptr + pid_b * stride_vb + pid_kh * stride_vh,
        shape=(K_LEN, HEAD_DIM),
        strides=(stride_vn, stride_vd),
        offsets=(0, 0),
        block_shape=(BLOCK_SIZE_K, HEAD_DIM),
        order=(1, 0),
    )
    # load q
    q = tl.load(q_ptrs, boundary_check=(0, 1), padding_option="zero")
    # init statistics
    off_m = tl.arange(0, BLOCK_SIZE_Q) + pid_q * BLOCK_SIZE_Q
    off_n = tl.arange(0, BLOCK_SIZE_K)
    m_i = tl.full((BLOCK_SIZE_Q,), float("-inf"), dtype=tl.float32)
    lse_i = tl.full((BLOCK_SIZE_Q,), float("-inf"), dtype=tl.float32)
    acc_o = tl.full((BLOCK_SIZE_Q, HEAD_DIM), 0, dtype=tl.float32)
    # flash attention
    # block_cac_cnt = 0 # for spa cac
    while v_count < v_num or s_count < s_num: # 
        # block_cac_cnt += 1
        # get v_block_id and s_block_id
        if v_count < v_num:
            v_block_id = tl.load(v_idx_ptr)
        else:
            v_block_id = 2147483647
        if s_count < s_num:
            s_block_id = pid_q - tl.load(s_idx_ptr) # slash offset current from the end
        else:
            s_block_id = 2147483647
        # get current block start position
        if v_block_id < s_block_id:
            block_start = v_block_id * BLOCK_SIZE_K
        else:
            block_start = s_block_id * BLOCK_SIZE_K  # from loop to here can see that in all cac the max(v_num, s_num) blocks
        # load k
        k = tl.load(
            tl.advance(k_ptrs, (0, block_start)),
            boundary_check=(1,),
            padding_option="zero",
        )
        # compute qk
        qk = tl.zeros((BLOCK_SIZE_Q, BLOCK_SIZE_K), dtype=tl.float32)
        qk += tl.where((block_start + off_n)[None, :] >= 0, 0, float("-inf"))
        qk += tl.where(
            off_m[:, None] >= (block_start + off_n)[None, :], 0, float("-inf")
        )
        qk += tl.dot(q, k) * softmax_scale
        # compute m_ij and l_ij
        m_ij = tl.maximum(m_i, tl.max(qk, axis=1))
        p = tl.math.exp2(qk - m_ij[:, None])
        l_ij = tl.sum(p, axis=1)
        # scale acc_o
        acc_o_scale = tl.math.exp2(m_i - m_ij)
        acc_o = acc_o * acc_o_scale[:, None]
        # load v and update acc_o
        v = tl.load(
            tl.advance(v_ptrs, (block_start, 0)),
            boundary_check=(0,),
            padding_option="zero",
        )
        p = p.to(v.dtype)
        acc_o += tl.dot(p, v)
        # update statistics
        m_i = m_ij
        lse_i = m_ij + tl.math.log2(tl.math.exp2(lse_i - m_ij) + l_ij)
        # update v_count, s_count, v_idx_ptr, s_idx_ptr
        if v_block_id < s_block_id:
            v_count += 1
            v_idx_ptr += stride_vit
        elif v_block_id > s_block_id:
            s_count += 1
            s_idx_ptr -= stride_sit
        else:
            v_count += 1
            s_count += 1
            v_idx_ptr += stride_vit
            s_idx_ptr -= stride_sit
    # final scale
    acc_o = acc_o * tl.math.exp2(m_i - lse_i)[:, None]
    # save output
    o_ptrs = tl.make_block_ptr(
        base=o_ptr + pid_b * stride_ob + pid_h * stride_oh,
        shape=(Q_LEN, HEAD_DIM),
        strides=(stride_on, stride_od),
        offsets=(pid_q * BLOCK_SIZE_Q, 0),
        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),
        order=(1, 0),
    )
    tl.store(o_ptrs, acc_o.to(tl.bfloat16), boundary_check=(0,))
    # tl.store(v_num_ptr + pid_q * stride_vnt, block_cac_cnt.to(v_num_ptr.type.element_ty))

@triton.jit
def vertical_slash_attention_spar_return_kernel(
    q_ptr,  # shape: [batch_size, seq_len, num_q_heads, head_dim]
    k_ptr,  # shape: [batch_size, seq_len, num_kv_heads, head_dim]
    v_ptr,  # shape: [batch_size, seq_len, num_kv_heads, head_dim]
    o_ptr,  # shape: [batch_size, seq_len, num_q_heads, head_dim]
    v_idx_ptr,  # shape: [batch_size, num_heads, num_v_blocks]
    v_num_ptr,  # shape: [batch_size, num_heads, num_rows]
    s_idx_ptr,  # shape: [batch_size, num_heads, num_rows]
    s_num_ptr,  # shape: [batch_size, num_heads]
    # shape
    BATCH_SIZE,
    NUM_Q_HEADS,
    NUM_KV_HEADS,
    GQA_GROUPS,
    Q_LEN,
    K_LEN,
    HEAD_DIM: tl.constexpr,
    # softmax_scale
    softmax_scale,
    # gqa
    gqa_interleave: tl.constexpr,
    # stride
    stride_qb,
    stride_qn,
    stride_qh,
    stride_qd,
    stride_kb,
    stride_kn,
    stride_kh,
    stride_kd,
    stride_vb,
    stride_vn,
    stride_vh,
    stride_vd,
    stride_ob,
    stride_on,
    stride_oh,
    stride_od,
    # vertical index stride
    stride_vib,
    stride_vih,
    stride_vit,
    stride_vnb,
    stride_vnh,
    stride_vnt,
    # slash index stride
    stride_sib,
    stride_sih,
    stride_sit,
    stride_snb,
    stride_snh,
    stride_snt,
    # META parameters
    BLOCK_SIZE_Q: tl.constexpr,  # q block size
    BLOCK_SIZE_K: tl.constexpr,  # k block size

):
    tl.static_assert(BLOCK_SIZE_Q == BLOCK_SIZE_K)
    pid_b = tl.program_id(1)
    pid_h = tl.program_id(2)
    if gqa_interleave:
        pid_kh = pid_h % NUM_KV_HEADS
    else:
        pid_kh = pid_h // GQA_GROUPS
    pid_q = tl.program_id(0)
    # get vertical block num and slash block num
    v_num_ptr = v_num_ptr + pid_b * stride_vnb + pid_h * stride_vnh
    v_num = tl.load(v_num_ptr + pid_q * stride_vnt)
    s_num_ptr = s_num_ptr + pid_b * stride_snb + pid_h * stride_snh
    s_num = tl.load(s_num_ptr + pid_q * stride_snt)
    # init v_count and s_count
    v_idx_ptr = v_idx_ptr + pid_b * stride_vib + pid_h * stride_vih
    s_idx_ptr = (
        s_idx_ptr + pid_b * stride_sib + pid_h * stride_sih + (s_num - 1) * stride_sit
    )
    v_count = 0
    s_count = 0
    # init qkv ptrs
    q_ptrs = tl.make_block_ptr(
        base=q_ptr + pid_b * stride_qb + pid_h * stride_qh,
        shape=(Q_LEN, HEAD_DIM),
        strides=(stride_qn, stride_qd),
        offsets=(pid_q * BLOCK_SIZE_Q, 0),
        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),
        order=(1, 0),
    )
    k_ptrs = tl.make_block_ptr(
        base=k_ptr + pid_b * stride_kb + pid_kh * stride_kh,
        shape=(HEAD_DIM, K_LEN),
        strides=(stride_kd, stride_kn),
        offsets=(0, 0),
        block_shape=(HEAD_DIM, BLOCK_SIZE_K),
        order=(0, 1),
    )
    v_ptrs = tl.make_block_ptr(
        base=v_ptr + pid_b * stride_vb + pid_kh * stride_vh,
        shape=(K_LEN, HEAD_DIM),
        strides=(stride_vn, stride_vd),
        offsets=(0, 0),
        block_shape=(BLOCK_SIZE_K, HEAD_DIM),
        order=(1, 0),
    )
    # load q
    q = tl.load(q_ptrs, boundary_check=(0, 1), padding_option="zero")
    # init statistics
    off_m = tl.arange(0, BLOCK_SIZE_Q) + pid_q * BLOCK_SIZE_Q
    off_n = tl.arange(0, BLOCK_SIZE_K)
    m_i = tl.full((BLOCK_SIZE_Q,), float("-inf"), dtype=tl.float32)
    lse_i = tl.full((BLOCK_SIZE_Q,), float("-inf"), dtype=tl.float32)
    acc_o = tl.full((BLOCK_SIZE_Q, HEAD_DIM), 0, dtype=tl.float32)
    # flash attention
    block_cac_cnt = 0 # for spa cac
    while v_count < v_num or s_count < s_num: # 
        block_cac_cnt += 1
        # get v_block_id and s_block_id
        if v_count < v_num:
            v_block_id = tl.load(v_idx_ptr)
        else:
            v_block_id = 2147483647
        if s_count < s_num:
            s_block_id = pid_q - tl.load(s_idx_ptr) # slash offset current from the end
        else:
            s_block_id = 2147483647
        # get current block start position
        if v_block_id < s_block_id:
            block_start = v_block_id * BLOCK_SIZE_K
        else:
            block_start = s_block_id * BLOCK_SIZE_K  # from loop to here can see that in all cac the max(v_num, s_num) blocks
        # load k
        k = tl.load(
            tl.advance(k_ptrs, (0, block_start)),
            boundary_check=(1,),
            padding_option="zero",
        )
        # compute qk
        qk = tl.zeros((BLOCK_SIZE_Q, BLOCK_SIZE_K), dtype=tl.float32)
        qk += tl.where((block_start + off_n)[None, :] >= 0, 0, float("-inf"))
        qk += tl.where(
            off_m[:, None] >= (block_start + off_n)[None, :], 0, float("-inf")
        )
        qk += tl.dot(q, k) * softmax_scale
        # compute m_ij and l_ij
        m_ij = tl.maximum(m_i, tl.max(qk, axis=1))
        p = tl.math.exp2(qk - m_ij[:, None])
        l_ij = tl.sum(p, axis=1)
        # scale acc_o
        acc_o_scale = tl.math.exp2(m_i - m_ij)
        acc_o = acc_o * acc_o_scale[:, None]
        # load v and update acc_o
        v = tl.load(
            tl.advance(v_ptrs, (block_start, 0)),
            boundary_check=(0,),
            padding_option="zero",
        )
        p = p.to(v.dtype)
        acc_o += tl.dot(p, v)
        # update statistics
        m_i = m_ij
        lse_i = m_ij + tl.math.log2(tl.math.exp2(lse_i - m_ij) + l_ij)
        # update v_count, s_count, v_idx_ptr, s_idx_ptr
        if v_block_id < s_block_id:
            v_count += 1
            v_idx_ptr += stride_vit
        elif v_block_id > s_block_id:
            s_count += 1
            s_idx_ptr -= stride_sit
        else:
            v_count += 1
            s_count += 1
            v_idx_ptr += stride_vit
            s_idx_ptr -= stride_sit
    # final scale
    acc_o = acc_o * tl.math.exp2(m_i - lse_i)[:, None]
    # save output
    o_ptrs = tl.make_block_ptr(
        base=o_ptr + pid_b * stride_ob + pid_h * stride_oh,
        shape=(Q_LEN, HEAD_DIM),
        strides=(stride_on, stride_od),
        offsets=(pid_q * BLOCK_SIZE_Q, 0),
        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),
        order=(1, 0),
    )
    tl.store(o_ptrs, acc_o.to(tl.bfloat16), boundary_check=(0,))
    tl.store(v_num_ptr + pid_q * stride_vnt, block_cac_cnt.to(v_num_ptr.type.element_ty))

def pad_and_sort_index(
    idx: Union[torch.Tensor, List[List[torch.Tensor]]], pad_value: int
):
    if not isinstance(idx, torch.Tensor):
        assert (
            isinstance(idx, list)
            and isinstance(idx[0], list)
            and isinstance(idx[0][0], torch.Tensor)
            and len(idx[0][0].shape) == 1
        )
        batch_size = len(idx)
        num_heads = len(idx[0])
        idx = [item.view(-1, 1) for sublist in idx for item in sublist]
        idx = torch.nn.utils.rnn.pad_sequence(
            idx,
            batch_first=True,
            padding_value=pad_value,
        ).view(batch_size, num_heads, -1)
    else:
        assert len(idx.shape) == 3
    idx = idx.sort(-1).values
    return idx


def triton_vertical_slash_attention(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    block_size: int,
    vertical_index: Union[torch.Tensor, List[List[torch.Tensor]]],
    slash_index: Union[torch.Tensor, List[List[torch.Tensor]]],
    softmax_scale: Optional[float] = None,
    gqa_interleave: bool = False,
    return_computational_ratio: bool = False,
):
    batch_size, q_len, num_q_heads, head_dim = q.shape
    batch_size, k_len, num_kv_heads, head_dim = k.shape
    assert q.dtype == torch.bfloat16
    assert q_len == k_len
    assert head_dim in {16, 32, 64, 128}, "only support head_dim in {16, 32, 64, 128}"
    assert block_size in {
        32,
        64,
        128,
    }, "only support block size in {16, 32, 64, 128}"
    num_blocks = triton.cdiv(q_len, block_size)
    # gqa
    assert num_q_heads % num_kv_heads == 0
    gqa_groups = num_q_heads // num_kv_heads
    # softmax_scale
    if softmax_scale is None:
        softmax_scale = 1 / math.sqrt(head_dim) * math.log2(math.e)
    else:
        softmax_scale = softmax_scale * math.log2(math.e)
    # pad idx if get list[list[tensor]]
    vertical_index = pad_and_sort_index(vertical_index, 2147483647)
    slash_index = pad_and_sort_index(slash_index, 2147483647)
    
    # get vertical and slash block num for each row
    vertical_index = vertical_index.to(torch.int32)
    slash_index = slash_index.to(torch.int32)
    # print(f"vertical_index:{vertical_index}")
    # print(f"slash_index:{slash_index}")
    vertical_num_blocks, slash_num_blocks = get_vertical_slash_block_num(
        vertical_index, slash_index, num_blocks
    )
    Logger.log(f"vertical_num_blocks:{vertical_num_blocks.shape}") 
    Logger.log(f"slash_num_blocks:{slash_num_blocks.shape}")
 
    o = torch.empty_like(q)
    num_warps = 8
    num_stages = 3 if block_size >= 128 else 5
    if return_computational_ratio:  
        vertical_slash_attention_spar_return_kernel[(num_blocks, batch_size, num_q_heads)](
            q,
            k,
            v,
            o,
            vertical_index,
            vertical_num_blocks,
            slash_index,
            slash_num_blocks,
            batch_size,
            num_q_heads,
            num_kv_heads,
            gqa_groups,
            q_len,
            k_len,
            head_dim,
            softmax_scale,
            gqa_interleave,
            q.stride(0),
            q.stride(1),
            q.stride(2),
            q.stride(3),
            k.stride(0),
            k.stride(1),
            k.stride(2),
            k.stride(3),
            v.stride(0),
            v.stride(1),
            v.stride(2),
            v.stride(3),
            o.stride(0),
            o.stride(1),
            o.stride(2),
            o.stride(3),
            vertical_index.stride(0),
            vertical_index.stride(1),
            vertical_index.stride(2),
            vertical_num_blocks.stride(0),
            vertical_num_blocks.stride(1),
            vertical_num_blocks.stride(2),
            slash_index.stride(0),
            slash_index.stride(1),
            slash_index.stride(2),
            slash_num_blocks.stride(0),
            slash_num_blocks.stride(1),
            slash_num_blocks.stride(2),
            BLOCK_SIZE_Q=block_size,
            BLOCK_SIZE_K=block_size,
            num_warps=num_warps,
            num_stages=num_stages,
        )
        tot_cac_blocks = vertical_num_blocks.sum(dim=-1).item() #[1,1,1]
        Logger.log(f"num_blocks:{num_blocks}")
        Logger.log(f"tot_cac_blocks:{tot_cac_blocks}")
        tot_blocks = num_blocks * (num_blocks + 1) / 2
        cac_ratio = tot_cac_blocks / tot_blocks
        return o, cac_ratio
    
    
    vertical_slash_attention_kernel[(num_blocks, batch_size, num_q_heads)](
        q,
        k,
        v,
        o,
        vertical_index,
        vertical_num_blocks,
        slash_index,
        slash_num_blocks,
        batch_size,
        num_q_heads,
        num_kv_heads,
        gqa_groups,
        q_len,
        k_len,
        head_dim,
        softmax_scale,
        gqa_interleave,
        q.stride(0),
        q.stride(1),
        q.stride(2),
        q.stride(3),
        k.stride(0),
        k.stride(1),
        k.stride(2),
        k.stride(3),
        v.stride(0),
        v.stride(1),
        v.stride(2),
        v.stride(3),
        o.stride(0),
        o.stride(1),
        o.stride(2),
        o.stride(3),
        vertical_index.stride(0),
        vertical_index.stride(1),
        vertical_index.stride(2),
        vertical_num_blocks.stride(0),
        vertical_num_blocks.stride(1),
        vertical_num_blocks.stride(2),
        slash_index.stride(0),
        slash_index.stride(1),
        slash_index.stride(2),
        slash_num_blocks.stride(0),
        slash_num_blocks.stride(1),
        slash_num_blocks.stride(2),
        BLOCK_SIZE_Q=block_size,
        BLOCK_SIZE_K=block_size,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    return o


def transform_veritcal_slash_idx(v_idx, s_idx, num_blocks):
    batch_size, num_heads, _ = v_idx.shape
    range_blocks = torch.arange(num_blocks, device=s_idx.device)[None, None, :, None]
    # vertical
    v_idx = (
        torch.arange(0, num_blocks, device=v_idx.device)[None, None, :, None]
        * num_blocks
        + v_idx[:, :, None, :]
    ).view(batch_size, num_heads, -1)
    v_idx[v_idx // num_blocks < v_idx % num_blocks] = 0
    # slash
    s_idx = (
        range_blocks * num_blocks + range_blocks + s_idx[:, :, None, :] * num_blocks
    ).view(batch_size, num_heads, -1)
    s_idx[s_idx >= num_blocks * num_blocks] = 0
    # union
    vs_idx = torch.cat((s_idx, v_idx), dim=-1)
    block_idx = [
        [torch.unique(vs_idx[b, h]) for h in range(num_heads)]
        for b in range(batch_size)
    ]
    return block_idx


def triton_vertical_slash_attention_use_block_wise_attention(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    block_size: int,
    vertical_index: torch.Tensor,
    slash_index: torch.Tensor,
    softmax_scale: Optional[float] = None,
    gqa_interleave: bool = False,
):
    vertical_index = pad_and_sort_index(vertical_index, 0)
    slash_index = pad_and_sort_index(slash_index, 0)
    num_blocks = math.ceil(q.shape[1] / block_size)
    block_idx = transform_veritcal_slash_idx(vertical_index, slash_index, num_blocks)
    return triton_block_wise_attention(
        q,
        k,
        v,
        block_idx,
        block_size,
        softmax_scale=softmax_scale,
        gqa_interleave=gqa_interleave,
    )


def get_random_vertical_slash_index(
    batch_size, num_heads, num_blocks, min_rate, max_rate
):
    min_blocks = math.ceil(min_rate * num_blocks)
    max_blocks = math.ceil(max_rate * num_blocks)
    vertical_index = []
    slash_index = []
    for b in range(batch_size):
        vertical_index.append([])
        slash_index.append([])
        for h in range(num_heads):
            v_size = random.randint(min_blocks, max_blocks)
            v_idx = torch.randn(num_blocks, device="cuda").argsort()[:v_size]
            v_idx[0] = 0
            v_idx = v_idx.unique().to(torch.int32)
            vertical_index[b].append(v_idx)

            s_size = random.randint(min_blocks, max_blocks)
            s_idx = torch.randn(num_blocks, device="cuda").argsort()[:s_size]
            s_idx[0] = 0
            s_idx = s_idx.unique().to(torch.int32)
            slash_index[b].append(s_idx)
    return vertical_index, slash_index


def torch_bhn_sumpool(x: torch.Tensor, kernel_size: int):
    b, h, n = x.shape
    x = torch.nn.functional.pad(
        x,
        (
            0,
            math.ceil(n / kernel_size) * kernel_size - n,
        ),
        value=0,
    )
    x = x.view(b, h, -1, kernel_size).sum(-1)
    return x


def sum_all_diagonal_matrix(mat: torch.tensor):
    b, h, n, m = mat.shape
    mat_padded = torch.nn.functional.pad(mat, (n - 1, 0), value=0)
    mat_strided = mat_padded.as_strided(
        (b, h, m, n), (h * n * (n + m - 1), n * (n + m - 1), 1, n + m)
    )
    sum_diags = torch.sum(mat_strided, -1)
    return sum_diags


def transform_veritcal_slash_idx(v_idx, s_idx, num_blocks):
    batch_size, num_heads, _ = v_idx.shape
    range_blocks = torch.arange(num_blocks, device=s_idx.device)[None, None, :, None]
    # vertical
    v_idx = (
        torch.arange(0, num_blocks, device=v_idx.device)[None, None, :, None]
        * num_blocks
        + v_idx[:, :, None, :]
    ).view(batch_size, num_heads, -1)
    v_idx[v_idx // num_blocks < v_idx % num_blocks] = 0
    # slash
    s_idx = (
        range_blocks * num_blocks + range_blocks + s_idx[:, :, None, :] * num_blocks
    ).view(batch_size, num_heads, -1)
    s_idx[s_idx >= num_blocks * num_blocks] = 0
    # union
    vs_idx = torch.cat((s_idx, v_idx), dim=-1)
    block_idx = [
        [torch.unique(vs_idx[b, h]) for h in range(num_heads)]
        for b in range(batch_size)
    ]
    return block_idx


causal_mask = None


def get_block_vertical_slash_from_qk(
    qk: torch.Tensor,
    block_size: int,
):
    # softmax
    qk = torch.nn.functional.softmax(qk, dim=-1, dtype=torch.float32)
    qk = rearrange(qk, "b h g i j -> b (h g) i j")
    batch_size, num_heads, last_q_len, seq_len = qk.shape
    # slash shape: [batch_size, num_heads, seq_len] -> [batch_size, num_heads, num_blocks]
    slash = sum_all_diagonal_matrix(qk)
    slash = torch_bhn_sumpool(slash, block_size)
    slash = slash / last_q_len
    # vertical shape: [batch_size, num_heads, seq_len] -> [batch_size, num_heads, num_blocks]
    vertical = qk.sum(-2)
    vertical = torch_bhn_sumpool(vertical, block_size)
    vertical = vertical / last_q_len
    return vertical, slash


def get_vertical_slash_topk(
    q,
    k,
    v,
    block_size,
    vertical_size,
    slash_size,
    gqa_interleave=False,
):
    batch_size, seq_len, num_heads, head_dim = q.shape
    gqa_groups = num_heads // k.shape[2]
    num_blocks = math.ceil(seq_len / block_size)
    # last qk attention, qk shape: [batch_size, num_heads, block_size, seq_len]
    q = q[:, -block_size:, :, :] / math.sqrt(head_dim)
    if not gqa_interleave:
        qk = torch.einsum(
            "bihgd, bjhgd -> bhgij",
            q.view(q.shape[0], q.shape[1], -1, gqa_groups, head_dim),
            k.view(k.shape[0], k.shape[1], -1, 1, head_dim),
        )
    else:
        qk = torch.einsum(
            "bihgd, bjhgd -> bhgij",
            q.view(q.shape[0], q.shape[1], gqa_groups, -1, head_dim),
            k.view(k.shape[0], k.shape[1], 1, -1, head_dim),
        )
    global causal_mask
    if causal_mask is None:
        causal_mask = torch.arange(0, block_size, device=q.device)
        causal_mask = causal_mask[:, None] >= causal_mask[None, :]
        causal_mask = causal_mask[None, None, None, ...]
    qk[..., -block_size:].masked_fill_(
        ~causal_mask[..., :block_size, :block_size], float("-inf")
    )
    vertical, slash = get_block_vertical_slash_from_qk(qk, block_size)
    # keep first vertical and slash block
    vertical[..., :1] = torch.inf
    slash[..., -1:] = torch.inf
    # get slash topk
    slash = slash.view(batch_size * num_heads, -1)
    slash_topk = (num_blocks - 1) - slash.topk(min(slash_size, num_blocks), -1).indices
    slash_topk = slash_topk.view(batch_size, num_heads, -1)
    # get vertical topk
    vertical = vertical.view(batch_size * num_heads, -1)
    vertical_topk = vertical.topk(min(vertical_size, num_blocks), -1).indices
    vertical_topk = vertical_topk.view(batch_size, num_heads, -1)
    return vertical_topk, slash_topk


def vertical_slash_attention(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    block_size: int,
    vertical_size: int,
    slash_size: int,
    gqa_interleave: bool = False,
    softmax_scale: Optional[float] = None,
    return_computational_ratio: bool = False,
):
    batch_size, seq_len, num_heads, head_dim = q.shape
    assert head_dim in {16, 32, 64, 128}
    assert block_size in {16, 32, 64, 128}
    num_blocks = math.ceil(seq_len / block_size)
    vertical_size = math.ceil(vertical_size / block_size)
    slash_size = math.ceil(slash_size / block_size)

    # 
    # if seq_len <= min(vertical_size, slash_size) * block_size:
    #     return flash_attn_func(q, k, v, softmax_scale=softmax_scale, causal=True)
    # get vertical slash index
    vertical_topk, slash_topk = get_vertical_slash_topk(
        q,
        k,
        v,
        block_size,
        vertical_size,
        slash_size,
        gqa_interleave,
    )
    Logger.log(f"vertical_topk:{vertical_topk.shape}")
    Logger.log(f"slash_topk:{slash_topk.shape}")
    vertical_topk = [
        [vertical_topk[b, h].unique() for h in range(num_heads)]
        for b in range(batch_size)
    ]
    slash_topk = [
        [slash_topk[b, h].unique() for h in range(num_heads)]
        for b in range(batch_size)
    ]
    if return_computational_ratio:
        attn_out, cac_ratio = triton_vertical_slash_attention(
            q,
            k,
            v,
            block_size,
            vertical_topk,
            slash_topk,
            softmax_scale=softmax_scale,
            gqa_interleave=gqa_interleave,
            return_computational_ratio=True,
        )
        return attn_out.transpose(1,2), cac_ratio
    else: 
        attn_out = triton_vertical_slash_attention(
            q,
            k,
            v,
            block_size,
            vertical_topk,
            slash_topk,
            softmax_scale=softmax_scale,
            gqa_interleave=gqa_interleave,
        )
        return attn_out.transpose(1,2)


if __name__ == "__main__":
    torch.manual_seed(0)
    B, N, H, D = 1, 64000, 32, 128

    # 这里要指定数量
    vertical_size = 1024
    slash_size = 8192
    
    q = torch.randn(B, N, H, D, device="cuda:0", dtype=torch.bfloat16) / 0.5
    k = torch.randn(B, N, H // 4, D, device="cuda:0", dtype=torch.bfloat16) / 0.5
    v = torch.randn(B, N, H // 4, D, device="cuda:0", dtype=torch.bfloat16)

    vertical_slash_output = vertical_slash_attention(
        q,
        k,
        v,
        vertical_size=vertical_size,
        slash_size=slash_size,
        gqa_interleave=False,
        block_size=128,
    )
    print("attention output norm:", vertical_slash_output.norm())
