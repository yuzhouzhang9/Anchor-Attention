# Copyright 2024 ByteDance and/or its affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import triton
import triton.language as tl
import torch


@triton.jit
def rotate_half_kernel(
    x_ptr,
    y_ptr,
    seq_len,
    head_dim: tl.constexpr,
    half_dim: tl.constexpr,
    inplace: tl.constexpr,
    stride_xb,
    stride_xh,
    stride_xn,
    stride_xd,
    stride_yb,
    stride_yh,
    stride_yn,
    stride_yd,
    BLOCK_SIZE_N: tl.constexpr,
):
    pid_b = tl.program_id(0)
    pid_h = tl.program_id(1)
    pid_n = tl.program_id(2)
    x_ptrs = tl.make_block_ptr(
        x_ptr + pid_b * stride_xb + pid_h * stride_xh,
        shape=(seq_len, head_dim),
        strides=(stride_xn, stride_xd),
        block_shape=(BLOCK_SIZE_N, head_dim),
        offsets=(pid_n * BLOCK_SIZE_N, 0),
        order=(1, 0),
    )
    x = tl.load(x_ptrs, boundary_check=(0,), padding_option="zero")
    off_d = tl.arange(0, head_dim)
    mask = tl.full((1, head_dim), 1, dtype=x.dtype)
    mask += tl.where(off_d >= half_dim, -2, 0)
    x = x * mask
    x = tl.reshape(x, BLOCK_SIZE_N, 2, half_dim)
    x = tl.permute(x, 0, 2, 1)
    x = tl.flip(x, 2)
    x = tl.permute(x, 0, 2, 1)
    x = tl.reshape(x, BLOCK_SIZE_N, head_dim)
    if inplace:
        tl.store(x_ptrs, x.to(tl.bfloat16), boundary_check=(0,))
    else:
        y_ptrs = tl.make_block_ptr(
            y_ptr + pid_b * stride_yb + pid_h * stride_yh,
            shape=(seq_len, head_dim),
            strides=(stride_yn, stride_yd),
            block_shape=(BLOCK_SIZE_N, head_dim),
            offsets=(pid_n * BLOCK_SIZE_N, 0),
            order=(1, 0),
        )
        tl.store(y_ptrs, x.to(tl.bfloat16), boundary_check=(0,))


def triton_rotate_half(x: torch.Tensor, head_first: bool = True, inplace: bool = False):
    """same as rotate_half from transformers.models.llama.modeling_llama.rotate_half
    Args:
        x (torch.Tensor): shape [batch_size, num_heads, seqlen, head_dim], head_dim must be one of [32, 64, 128]
        inplace (bool, optional): Defaults to False.
    Returns:
        torch.Tensor: tensor after rotate
    """
    if head_first:
        batch_size, num_heads, seqlen, head_dim = x.shape
    else:
        batch_size, seqlen, num_heads, head_dim = x.shape
    assert head_dim in {16, 32, 64, 128}
    assert x.dtype == torch.bfloat16
    head_dim = triton.next_power_of_2(head_dim)
    half_dim = triton.next_power_of_2(head_dim // 2)
    if inplace:
        y = x
    else:
        y = torch.empty_like(x)
    BLOCK_SIZE_N = 128
    grid = lambda META: (
        batch_size,
        num_heads,
        triton.cdiv(seqlen, META["BLOCK_SIZE_N"]),
    )
    rotate_half_kernel[grid](
        x,
        y,
        seqlen,
        head_dim,
        half_dim,
        inplace,
        x.stride(0),
        x.stride(1) if head_first else x.stride(2),
        x.stride(2) if head_first else x.stride(1),
        x.stride(3),
        y.stride(0),
        y.stride(1) if head_first else y.stride(2),
        y.stride(2) if head_first else y.stride(1),
        y.stride(3),
        BLOCK_SIZE_N=BLOCK_SIZE_N,
    )
    return y


@triton.jit
def apply_rotary_pos_emb_kernel(
    x_ptr,  # shape [batch_size, num_heads, seqlen, head_dim]
    cos_ptr,  # shape [batch_size, seqlen, head_dim]
    sin_ptr,  # shape [batch_size, seqlen, head_dim]
    # shape
    seq_len,
    head_dim: tl.constexpr,
    half_dim: tl.constexpr,
    # stride
    stride_xb,
    stride_xh,
    stride_xn,
    stride_xd,
    stride_cb,
    stride_cn,
    stride_cd,
    stride_sb,
    stride_sn,
    stride_sd,
    # meta
    BLOCK_SIZE_N: tl.constexpr,
):
    pid_b = tl.program_id(0)
    pid_h = tl.program_id(1)
    pid_n = tl.program_id(2)
    # pointers
    x_ptrs = tl.make_block_ptr(
        x_ptr + pid_b * stride_xb + pid_h * stride_xh,
        shape=(seq_len, head_dim),
        strides=(stride_xn, stride_xd),
        block_shape=(BLOCK_SIZE_N, head_dim),
        offsets=(pid_n * BLOCK_SIZE_N, 0),
        order=(1, 0),
    )
    cos_ptrs = tl.make_block_ptr(
        cos_ptr + pid_b * stride_cb,
        shape=(seq_len, head_dim),
        strides=(stride_cn, stride_cd),
        block_shape=(BLOCK_SIZE_N, head_dim),
        offsets=(pid_n * BLOCK_SIZE_N, 0),
        order=(1, 0),
    )
    sin_ptrs = tl.make_block_ptr(
        sin_ptr + pid_b * stride_sb,
        shape=(seq_len, head_dim),
        strides=(stride_sn, stride_sd),
        block_shape=(BLOCK_SIZE_N, head_dim),
        offsets=(pid_n * BLOCK_SIZE_N, 0),
        order=(1, 0),
    )
    # load data
    x = tl.load(x_ptrs, boundary_check=(0,), padding_option="zero")
    cos = tl.load(cos_ptrs, boundary_check=(0,), padding_option="zero")
    sin = tl.load(sin_ptrs, boundary_check=(0,), padding_option="zero")
    # rotate half
    off_d = tl.arange(0, head_dim)
    rot_mask = tl.full((1, head_dim), 1, dtype=x.dtype)
    rot_mask += tl.where(off_d >= half_dim, -2, 0)
    x_rot = x * rot_mask
    x_rot = tl.reshape(x_rot, BLOCK_SIZE_N, 2, half_dim)
    x_rot = tl.permute(x_rot, 0, 2, 1)
    x_rot = tl.flip(x_rot, 2)
    x_rot = tl.permute(x_rot, 0, 2, 1)
    x_rot = tl.reshape(x_rot, BLOCK_SIZE_N, head_dim)
    # rope
    x = x * cos + x_rot * sin
    # store
    tl.store(x_ptrs, x.to(tl.bfloat16), boundary_check=(0,))


def triton_apply_rotary_pos_emb(
    q, k, cos, sin, position_ids=None, unsqueeze_dim=1, head_first: bool = True
):
    """Applies Rotary Position Embedding to the query and key tensors INPLACE. Same as from transformers.models.llama.modeling_llama.apply_rotary_pos_emb
    Args:
        q (`torch.Tensor`): The query tensor. shape [batch_size, num_heads, seqlen, head_dim]
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding. shape [batch_size, seqlen, head_dim]
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*): unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1): unused.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    if head_first:
        batch_size, num_q_head, q_len, head_dim = q.shape
        batch_size, num_k_head, k_len, head_dim = k.shape
    else:
        batch_size, q_len, num_q_head, head_dim = q.shape
        batch_size, k_len, num_k_head, head_dim = k.shape
    assert head_dim in {32, 64, 128}
    assert q.dtype == torch.bfloat16
    assert k.dtype == torch.bfloat16
    # launch parameters
    head_dim = triton.next_power_of_2(head_dim)
    half_dim = triton.next_power_of_2(head_dim // 2)
    BLOCK_SIZE_N = 128
    q_grid = lambda META: (
        batch_size,
        num_q_head,
        triton.cdiv(q_len, META["BLOCK_SIZE_N"]),
    )
    k_grid = lambda META: (
        batch_size,
        num_k_head,
        triton.cdiv(q_len, META["BLOCK_SIZE_N"]),
    )
    num_warps = 4 if head_dim <= 64 else 8
    num_stages = 3
    # query
    apply_rotary_pos_emb_kernel[q_grid](
        q,
        cos,
        sin,
        q_len,
        head_dim,
        half_dim,
        q.stride(0),
        q.stride(1) if head_first else q.stride(2),
        q.stride(2) if head_first else q.stride(1),
        q.stride(3),
        cos.stride(0),
        cos.stride(1),
        cos.stride(2),
        sin.stride(0),
        sin.stride(1),
        sin.stride(2),
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    # key
    apply_rotary_pos_emb_kernel[k_grid](
        k,
        cos,
        sin,
        k_len,
        head_dim,
        half_dim,
        k.stride(0),
        k.stride(1) if head_first else k.stride(2),
        k.stride(2) if head_first else k.stride(1),
        k.stride(3),
        cos.stride(0),
        cos.stride(1),
        cos.stride(2),
        sin.stride(0),
        sin.stride(1),
        sin.stride(2),
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    return q, k


if __name__ == "__main__":
    from transformers.models.llama.modeling_llama import (
        rotate_half,
        apply_rotary_pos_emb,
    )
    from termcolor import colored

    B, H, N, D = 1, 32, 512 * 1024, 128
    X = torch.randn(B, H, N, D, device="cuda").bfloat16()
    Q = torch.randn(B, H, N, D, device="cuda").bfloat16()
    K = torch.randn(B, H // 4, N, D, device="cuda").bfloat16()
    cos = torch.rand(B, N, D, device="cuda").bfloat16() * 2 - 1
    sin = torch.rand(B, N, D, device="cuda").bfloat16() * 2 - 1

    print("** rotate half: **")
    output_torch = rotate_half(X)
    output_triton = triton_rotate_half(X, True)
    if torch.allclose(output_torch, output_triton, rtol=1e-2, atol=1e-2):
        print(colored("triton output == torch output:", "green"))
    else:
        print(colored("triton output != torch output:", "red"))
    print()

    print("** apply rope: **")
    q_torch, k_torch = apply_rotary_pos_emb(Q, K, cos, sin)
    q_triton, k_triton = triton_apply_rotary_pos_emb(Q, K, cos, sin)
    if torch.allclose(q_torch, q_triton, rtol=1e-2, atol=1e-2) and torch.allclose(
        k_torch, k_triton, rtol=1e-2, atol=1e-2
    ):
        print(colored("triton output == torch output:", "green"))
    else:
        print(colored("triton output != torch output:", "red"))
    print()

    # benchmark
    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=["N"],
            x_vals=[1] + [1024 * 2**i for i in range(1, 10)],
            line_arg="provider",
            line_vals=["torch", "triton"],
            line_names=[
                "Torch",
                "Triton",
            ],
            styles=[("green", "-"), ("blue", "-")],
            ylabel="ms",
            plot_name="** apply rope performance (ms) **",
            args={"B": 1, "H": 32, "D": 128},
        )
    )
    def benchmark(B, N, H, D, provider):
        k = torch.randn((B, H, N, D), device="cuda", dtype=torch.bfloat16)

        quantiles = [0.5, 0.2, 0.8]
        if provider == "torch":
            ms, min_ms, max_ms = triton.testing.do_bench(
                lambda: rotate_half(k), quantiles=quantiles
            )
        if provider == "triton":
            ms, min_ms, max_ms = triton.testing.do_bench(
                lambda: triton_rotate_half(k), quantiles=quantiles
            )
        return ms, min_ms, max_ms

    benchmark.run(show_plots=True, print_data=True)
    print()
